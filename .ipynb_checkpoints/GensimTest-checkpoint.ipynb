{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ead0bde3",
   "metadata": {},
   "source": [
    "# Gensim学习"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dae37233",
   "metadata": {},
   "source": [
    "使用list迭代的方式，将一段corpus转换称对应的dictionary。单词与其序列的映射关系。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4b4c79ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['human', 'interface', 'computer'],\n",
      " ['survey', 'user', 'computer', 'system', 'response', 'time'],\n",
      " ['eps', 'user', 'interface', 'system'],\n",
      " ['system', 'human', 'system', 'eps'],\n",
      " ['user', 'response', 'time'],\n",
      " ['trees'],\n",
      " ['graph', 'trees'],\n",
      " ['graph', 'minors', 'trees'],\n",
      " ['graph', 'minors', 'survey']]\n",
      "预处理语料库长度： 9\n"
     ]
    }
   ],
   "source": [
    "import pprint\n",
    "\n",
    "\n",
    "text_corpus = [\n",
    "    \"Human machine interface for lab abc computer applications\",\n",
    "    \"A survey of user opinion of computer system response time\",\n",
    "    \"The EPS user interface management system\",\n",
    "    \"System and human system engineering testing of EPS\",\n",
    "    \"Relation of user perceived response time to error measurement\",\n",
    "    \"The generation of random binary unordered trees\",\n",
    "    \"The intersection graph of paths in trees\",\n",
    "    \"Graph minors IV Widths of trees and well quasi ordering\",\n",
    "    \"Graph minors A survey\",\n",
    "]\n",
    "\n",
    "# 创建一个集合 用来存储一些介词\n",
    "stoplist = set('for a of the and to in'.split(' '))\n",
    "\n",
    "# print(type(stoplist))\n",
    "\n",
    "# .lower() 将文本转换成小写字母 .split() 对document进行分词\n",
    "# 最后通过if来排除常见的介词\n",
    "texts = [\n",
    "    [word for word in document.lower().split() if word not in stoplist]\n",
    "    for document in text_corpus\n",
    "]\n",
    "\n",
    "# 计算单词出现的频率\n",
    "from collections import defaultdict\n",
    "frequency = defaultdict(int)\n",
    "for text in texts:\n",
    "    for token in text:\n",
    "        frequency[token] += 1\n",
    "\n",
    "# 输出不止出现一次的单词\n",
    "# 通过双重循环 将二维数组转换为一维数组\n",
    "processed_corpus = [\n",
    "    [ token for token in text if frequency[token]> 1 ] for text in texts\n",
    "]\n",
    "\n",
    "pprint.pprint(processed_corpus)\n",
    "print('预处理语料库长度：',len(processed_corpus))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fb91226",
   "metadata": {},
   "source": [
    "使用gensim内的ditionary生成字典"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "02e3e5ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary<12 unique tokens: ['computer', 'human', 'interface', 'response', 'survey']...>\n",
      "{'computer': 0,\n",
      " 'eps': 8,\n",
      " 'graph': 10,\n",
      " 'human': 1,\n",
      " 'interface': 2,\n",
      " 'minors': 11,\n",
      " 'response': 3,\n",
      " 'survey': 4,\n",
      " 'system': 5,\n",
      " 'time': 6,\n",
      " 'trees': 9,\n",
      " 'user': 7}\n"
     ]
    }
   ],
   "source": [
    "from gensim import corpora\n",
    "\n",
    "dictionary = corpora.Dictionary(processed_corpus)\n",
    "print(dictionary)\n",
    "pprint.pprint(dictionary.token2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e9e3ea3a",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dictionary' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m--------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m new_doc \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHuman computer interation\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# new_doc = \"Human use computer, Human eps computer\"\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m new_vec \u001b[38;5;241m=\u001b[39m \u001b[43mdictionary\u001b[49m\u001b[38;5;241m.\u001b[39mdoc2bow(new_doc\u001b[38;5;241m.\u001b[39mlower()\u001b[38;5;241m.\u001b[39msplit())\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(new_vec)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'dictionary' is not defined"
     ]
    }
   ],
   "source": [
    "new_doc = \"Human computer interation\"\n",
    "# new_doc = \"Human use computer, Human eps computer\"\n",
    "\n",
    "new_vec = dictionary.doc2bow(new_doc.lower().split())\n",
    "print(new_vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6b0f8eb",
   "metadata": {},
   "source": [
    "在每个元组中，第一个数值表示单词的ID，第二个单词表示单词出现的次数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ef39b277",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dictionary' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m--------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m bow_corpus \u001b[38;5;241m=\u001b[39m [ dictionary\u001b[38;5;241m.\u001b[39mdoc2bow(text\u001b[38;5;241m.\u001b[39mlower()\u001b[38;5;241m.\u001b[39msplit()) \u001b[38;5;28;01mfor\u001b[39;00m text \u001b[38;5;129;01min\u001b[39;00m text_corpus]\n\u001b[0;32m      2\u001b[0m pprint\u001b[38;5;241m.\u001b[39mpprint(bow_corpus)\n",
      "Cell \u001b[1;32mIn[4], line 1\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[1;32m----> 1\u001b[0m bow_corpus \u001b[38;5;241m=\u001b[39m [ \u001b[43mdictionary\u001b[49m\u001b[38;5;241m.\u001b[39mdoc2bow(text\u001b[38;5;241m.\u001b[39mlower()\u001b[38;5;241m.\u001b[39msplit()) \u001b[38;5;28;01mfor\u001b[39;00m text \u001b[38;5;129;01min\u001b[39;00m text_corpus]\n\u001b[0;32m      2\u001b[0m pprint\u001b[38;5;241m.\u001b[39mpprint(bow_corpus)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'dictionary' is not defined"
     ]
    }
   ],
   "source": [
    "bow_corpus = [ dictionary.doc2bow(text.lower().split()) for text in text_corpus]\n",
    "pprint.pprint(bow_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e9b5dd9",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2a9e08f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(5, 0.5898341626740045), (11, 0.8075244024440723)]\n"
     ]
    }
   ],
   "source": [
    "from gensim import models\n",
    "\n",
    "tfidf = models.TfidfModel(bow_corpus)\n",
    "\n",
    "words = \"system minors\".lower().split()\n",
    "print(tfidf[dictionary.doc2bow(words)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80d80f14",
   "metadata": {},
   "source": [
    "每个元组中，第一个数值表示单词的ID，第二个表示tf-idf权重"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "96db9bf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query_bow\n",
      " [(5, 1)]\n",
      "[(0, 0.0), (1, 0.32448703), (2, 0.41707572), (3, 0.7184812), (4, 0.0), (5, 0.0), (6, 0.0), (7, 0.0), (8, 0.0)]\n"
     ]
    }
   ],
   "source": [
    "from gensim import similarities\n",
    "\n",
    "index = similarities.SparseMatrixSimilarity(tfidf[bow_corpus], num_features=12)\n",
    "\n",
    "query_document = 'system enginnering'.split()\n",
    "query_bow = dictionary.doc2bow(query_document)\n",
    "print('query_bow\\n', query_bow)\n",
    "\n",
    "sims = index[tfidf[query_bow]]\n",
    "print(list(enumerate(sims)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5d031740",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 0.7184812\n",
      "2 0.41707572\n",
      "1 0.32448703\n",
      "0 0.0\n",
      "4 0.0\n",
      "5 0.0\n",
      "6 0.0\n",
      "7 0.0\n",
      "8 0.0\n"
     ]
    }
   ],
   "source": [
    "for document_number, score in sorted(enumerate(sims), key=lambda x:x[1], reverse=True):\n",
    "    print(document_number, score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5c054688",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd09dbc5",
   "metadata": {},
   "source": [
    "### From Strings to Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "062c9255",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary<12 unique tokens: ['computer', 'human', 'interface', 'response', 'survey']...>\n"
     ]
    }
   ],
   "source": [
    "import pprint\n",
    "from collections import defaultdict\n",
    "\n",
    "# 导入9句话\n",
    "documents = [\n",
    "    \"Human machine interface for lab abc computer applications\",\n",
    "    \"A survey of user opinion of computer system response time\",\n",
    "    \"The EPS user interface management system\",\n",
    "    \"System and human system engineering testing of EPS\",\n",
    "    \"Relation of user perceived response time to error measurement\",\n",
    "    \"The generation of random binary unordered trees\",\n",
    "    \"The intersection graph of paths in trees\",\n",
    "    \"Graph minors IV Widths of trees and well quasi ordering\",\n",
    "    \"Graph minors A survey\",\n",
    "]\n",
    "\n",
    "stoplist = set('for a of the and to in'.split(' '))\n",
    "texts = [\n",
    "    [word for word in document.lower().split() if word not in stoplist ] \n",
    "    for document in documents\n",
    "]\n",
    "\n",
    "frequency = defaultdict(int)\n",
    "for text in texts:\n",
    "    for word in text:\n",
    "        # 使用方法相当于字典\n",
    "        frequency[word] += 1\n",
    "        \n",
    "# 去除只出现一token次的单词\n",
    "texts = [[token for token in text if frequency[token]>1 ] for text in texts]\n",
    "\n",
    "from gensim import corpora\n",
    "dictionary = corpora.Dictionary(texts)\n",
    "dictionary.save('./tmp/deerwester.dict')\n",
    "print(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ec8816ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'computer': 0, 'human': 1, 'interface': 2, 'response': 3, 'survey': 4, 'system': 5, 'time': 6, 'user': 7, 'eps': 8, 'trees': 9, 'graph': 10, 'minors': 11}\n",
      "new_vect:  \n",
      " [(0, 1), (1, 1)]\n"
     ]
    }
   ],
   "source": [
    "# 得到单词的ID\n",
    "print(dictionary.token2id)\n",
    "\n",
    "new_doc = \"human computer iteraction\"\n",
    "new_vect = dictionary.doc2bow(new_doc.lower().split())\n",
    "print('new_vect: ', '\\n', new_vect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "19e9ced0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[(0, 1), (1, 1), (2, 1)],\n",
      " [(0, 1), (3, 1), (4, 1), (5, 1), (6, 1), (7, 1)],\n",
      " [(2, 1), (5, 1), (7, 1), (8, 1)],\n",
      " [(1, 1), (5, 2), (8, 1)],\n",
      " [(3, 1), (6, 1), (7, 1)],\n",
      " [(9, 1)],\n",
      " [(9, 1), (10, 1)],\n",
      " [(9, 1), (10, 1), (11, 1)],\n",
      " [(4, 1), (10, 1), (11, 1)]]\n"
     ]
    }
   ],
   "source": [
    "corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "# 保存上述数据\n",
    "corpora.MmCorpus.serialize('./tmp/deerwester.mm', corpus)\n",
    "pprint.pprint(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "db0625ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from smart_open import open\n",
    "\n",
    "class MyCorpus:\n",
    "    def __iter__(self):\n",
    "        # 这里也可以通过http协议来访问网络上的文件\n",
    "        for line in open('./tmp/mycorpus.txt'):\n",
    "            # 通过迭代返回内容\n",
    "            yield dictionary.doc2bow(line.lower().split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8d5ccf50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<__main__.MyCorpus object at 0x00000182CB070FD0>\n",
      "[(0, 1), (1, 1), (2, 1)]\n",
      "[(0, 1), (3, 1), (4, 1), (5, 1), (6, 1), (7, 1)]\n",
      "[(2, 1), (5, 1), (7, 1), (8, 1)]\n",
      "[(1, 1), (5, 2), (8, 1)]\n",
      "[(3, 1), (6, 1), (7, 1)]\n",
      "[(9, 1)]\n",
      "[(9, 1), (10, 1)]\n",
      "[(9, 1), (10, 1), (11, 1)]\n",
      "[(4, 1), (10, 1), (11, 1)]\n"
     ]
    }
   ],
   "source": [
    "corpus_memory_friendly = MyCorpus()\n",
    "print(corpus_memory_friendly)\n",
    "\n",
    "# corpus现在是个对象 \n",
    "# 不能直接打印输出 要通过循环的方式遍历\n",
    "for vector in corpus_memory_friendly:\n",
    "    print(vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf9e14fe",
   "metadata": {},
   "source": [
    "虽然输出结果与上述相同，但是通过这样的方式，哦对内存是更加友好的。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "04647d1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary<12 unique tokens: ['computer', 'human', 'interface', 'response', 'survey']...>\n"
     ]
    }
   ],
   "source": [
    "# 将corpus中的所有单词构成一个字典\n",
    "dictionary = corpora.Dictionary(line.lower().split() for line in open('./tmp/mycorpus.txt'))\n",
    "# 获得常用符号的id\n",
    "stop_ids = [\n",
    "    dictionary.token2id[stopword]\n",
    "    for stopword in stoplist\n",
    "    if stopword in dictionary.token2id\n",
    "]\n",
    "# 获得只出现一次的id\n",
    "once_ids = [tokenid for tokenid, docfreq in dictionary.dfs.items()\n",
    "          if docfreq == 1]\n",
    "\n",
    "# 将上述id进行过滤\n",
    "dictionary.filter_tokens(stop_ids + once_ids)\n",
    "dictionary.compactify()\n",
    "print(dictionary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "956671d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MmCorpus(2 documents, 2 features, 1 non-zero entries)\n",
      "[[(1, 0.5)], []]\n"
     ]
    }
   ],
   "source": [
    "corpus = [[(1, 0.5)], []]\n",
    "# 保存一个Matrix Market文件\n",
    "corpora.MmCorpus.serialize('./tmp/corpus.mm', corpus)\n",
    "\n",
    "corpus = corpora.MmCorpus('./tmp/corpus.mm')\n",
    "print(corpus)\n",
    "print(list(corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "a1c41c8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[7 0]\n",
      " [6 3]\n",
      " [2 9]\n",
      " [1 8]\n",
      " [1 7]]\n",
      "[(0, 7.0), (1, 6.0), (2, 2.0), (3, 1.0), (4, 1.0)]\n",
      "[(1, 3.0), (2, 9.0), (3, 8.0), (4, 7.0)]\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "import numpy as np\n",
    "numpy_matrix = np.random.randint(10, size=[5, 2])\n",
    "corpus = gensim.matutils.Dense2Corpus(numpy_matrix)\n",
    "print(numpy_matrix)\n",
    "\n",
    "for doc in corpus:\n",
    "    print(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cc795fd",
   "metadata": {},
   "source": [
    "## Topics and Transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "6dd73791",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint\n",
    "from collections import defaultdict\n",
    "from gensim import corpora\n",
    "\n",
    "# 导入9句话\n",
    "documents = [\n",
    "    \"Human machine interface for lab abc computer applications\",\n",
    "    \"A survey of user opinion of computer system response time\",\n",
    "    \"The EPS user interface management system\",\n",
    "    \"System and human system engineering testing of EPS\",\n",
    "    \"Relation of user perceived response time to error measurement\",\n",
    "    \"The generation of random binary unordered trees\",\n",
    "    \"The intersection graph of paths in trees\",\n",
    "    \"Graph minors IV Widths of trees and well quasi ordering\",\n",
    "    \"Graph minors A survey\",\n",
    "]\n",
    "\n",
    "stoplist = set('for a of the and to in'.split(' '))\n",
    "texts = [\n",
    "    [word for word in document.lower().split() if word not in stoplist ] \n",
    "    for document in documents\n",
    "]\n",
    "\n",
    "frequency = defaultdict(int)\n",
    "for text in texts:\n",
    "    for word in text:\n",
    "        # 使用方法相当于字典\n",
    "        frequency[word] += 1\n",
    "        \n",
    "# 去除只出现一token次的单词\n",
    "texts = [[token for token in text if frequency[token]>1 ] for text in texts]\n",
    "\n",
    "dictionary = corpora.Dictionary(texts)\n",
    "corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "\n",
    "from gensim import models\n",
    "\n",
    "tfidf = models.TfidfModel(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "cc52ee34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0.7071067811865476), (1, 0.7071067811865476)]\n"
     ]
    }
   ],
   "source": [
    "doc_bow = [(0, 1), (1, 1)]\n",
    "print(tfidf[doc_bow])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64434473",
   "metadata": {},
   "source": [
    "`tfidf`可以用于将任何矩阵从旧的表示方法（单词的出现次数-整形）转换为新的表示方法（tfidf权重-浮点型）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "b49c3a7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0.5773502691896257), (1, 0.5773502691896257), (2, 0.5773502691896257)]\n",
      "[(0, 0.44424552527467476), (3, 0.44424552527467476), (4, 0.44424552527467476), (5, 0.3244870206138555), (6, 0.44424552527467476), (7, 0.3244870206138555)]\n",
      "[(2, 0.5710059809418182), (5, 0.4170757362022777), (7, 0.4170757362022777), (8, 0.5710059809418182)]\n",
      "[(1, 0.49182558987264147), (5, 0.7184811607083769), (8, 0.49182558987264147)]\n",
      "[(3, 0.6282580468670046), (6, 0.6282580468670046), (7, 0.45889394536615247)]\n",
      "[(9, 1.0)]\n",
      "[(9, 0.7071067811865475), (10, 0.7071067811865475)]\n",
      "[(9, 0.5080429008916749), (10, 0.5080429008916749), (11, 0.695546419520037)]\n",
      "[(4, 0.6282580468670046), (10, 0.45889394536615247), (11, 0.6282580468670046)]\n"
     ]
    }
   ],
   "source": [
    "corpus_tfidf = tfidf[corpus]\n",
    "for doc in corpus_tfidf:\n",
    "    print(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "88d4cc52",
   "metadata": {},
   "outputs": [],
   "source": [
    "lsi_model = models.LsiModel(corpus_tfidf, id2word=dictionary, num_topics=2)\n",
    "corpus_lsi = lsi_model[corpus_tfidf]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04218d35",
   "metadata": {},
   "source": [
    "通过Laten Semantic Indexing将Tf-Idf转换成一个latent 2-D空间。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "d5db7699",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.703*\"trees\" + 0.538*\"graph\" + 0.402*\"minors\" + 0.187*\"survey\" + 0.061*\"system\" + 0.060*\"response\" + 0.060*\"time\" + 0.058*\"user\" + 0.049*\"computer\" + 0.035*\"interface\"'),\n",
       " (1,\n",
       "  '-0.460*\"system\" + -0.373*\"user\" + -0.332*\"eps\" + -0.328*\"interface\" + -0.320*\"response\" + -0.320*\"time\" + -0.293*\"computer\" + -0.280*\"human\" + -0.171*\"survey\" + 0.161*\"trees\"')]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lsi_model.print_topics(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf038859",
   "metadata": {},
   "source": [
    "## Word2Vec 模型使用\n",
    "https://radimrehurek.com/gensim/auto_examples/tutorials/run_word2vec.html#sphx-glr-auto-examples-tutorials-run-word2vec-py\n",
    "\n",
    "传统方式：使用Bag of words\n",
    "* 从一些句子之中得到一些单词，对他们分别标号\n",
    "* 对于一条新的句子，按照单词序号记录单词出现的频率，生成一个表格\n",
    "\n",
    "这种方式虽然能够很容易进行，但是有不足之处\n",
    "1. 这种方式失去了句子的顺序，John likes Mary 和 Mary likes John\n",
    "2. 向量之间的距离并不反映语义上的相似度\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75859228",
   "metadata": {},
   "source": [
    "### Word2Vec demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fce38a0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[==================================================] 100.0% 1662.8/1662.8MB downloaded\n"
     ]
    }
   ],
   "source": [
    "import gensim.downloader as api\n",
    "wv = api.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3745d5ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word #0/3000000 is </s>\n",
      "word #1/3000000 is in\n",
      "word #2/3000000 is for\n",
      "word #3/3000000 is that\n",
      "word #4/3000000 is is\n",
      "word #5/3000000 is on\n",
      "word #6/3000000 is ##\n",
      "word #7/3000000 is The\n",
      "word #8/3000000 is with\n",
      "word #9/3000000 is said\n"
     ]
    }
   ],
   "source": [
    "for index, word in enumerate(wv.index_to_key):\n",
    "    if index == 10:\n",
    "        break\n",
    "    print(f\"word #{index}/{len(wv.index_to_key)} is {word}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4659b5cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The word 'cameroon does not appear in this model'\n"
     ]
    }
   ],
   "source": [
    "vec_king = wv['king']\n",
    "\n",
    "try:\n",
    "    vec_cameroon = wv['cameroon']\n",
    "except KeyError:\n",
    "    print(\"The word 'cameroon does not appear in this model'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6d4ed5b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'car'\t'minivan'\t0.69\n",
      "'car'\t'bicycle'\t0.54\n",
      "'car'\t'airplane'\t0.42\n",
      "'car'\t'cereal'\t0.14\n",
      "'car'\t'communism'\t0.06\n"
     ]
    }
   ],
   "source": [
    "pairs = [\n",
    "    ('car', 'minivan'),   # a minivan is a kind of car\n",
    "    ('car', 'bicycle'),   # still a wheeled vehicle\n",
    "    ('car', 'airplane'),  # ok, no wheels, but still a vehicle\n",
    "    ('car', 'cereal'),    # ... and so on\n",
    "    ('car', 'communism'),\n",
    "]\n",
    "for w1, w2 in pairs:\n",
    "    print('%r\\t%r\\t%.2f' % (w1, w2, wv.similarity(w1, w2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b5cc422e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('SUV', 0.853219211101532), ('vehicle', 0.8175785541534424), ('pickup_truck', 0.7763689160346985), ('Jeep', 0.7567334175109863), ('Ford_Explorer', 0.756571888923645)]\n"
     ]
    }
   ],
   "source": [
    "print(wv.most_similar(positive=['car', 'minivan'], topn=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0d9bbc02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "car\n"
     ]
    }
   ],
   "source": [
    "print(wv.doesnt_match(['fire', 'water', 'land', 'sea', 'air', 'car']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ec3b6e7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.22942671\n",
      "0.31618136\n",
      "[('princess', 0.6530032753944397), ('monarch', 0.6512453556060791), ('prince', 0.6426263451576233), ('kings', 0.6259569525718689), ('queens', 0.5816447138786316)]\n"
     ]
    }
   ],
   "source": [
    "print(wv.similarity('king', 'man'))\n",
    "print(wv.similarity('queen', 'woman'))\n",
    "\n",
    "print(wv.most_similar(positive=['king', 'man', 'queen'], topn=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceb790d1",
   "metadata": {},
   "source": [
    "## Training My Own Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c5ee33cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.test.utils import datapath\n",
    "from gensim import utils\n",
    "import pprint\n",
    "\n",
    "class MyCorpus:\n",
    "    def __iter__(self):\n",
    "        corpus_path = datapath('lee_background.cor')\n",
    "        for line in open(corpus_path):\n",
    "            yield utils.simple_preprocess(line)\n",
    "def test():\n",
    "    sentences = MyCorpus()\n",
    "    count = 0\n",
    "    num = 2\n",
    "    for sentence in sentences:\n",
    "        if count == 2:\n",
    "            break\n",
    "        pprint.pprint(sentence)\n",
    "        count += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f13342c",
   "metadata": {},
   "source": [
    "通过MyCorpus来训练模型。目前不要太担心这些训练参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b126942c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.models\n",
    "\n",
    "sentences = MyCorpus()\n",
    "model = gensim.models.Word2Vec(sentences=sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33deca5f",
   "metadata": {},
   "source": [
    "通过我们训练的模型 我们也能够实现上述demo的效果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "18eebef0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word #0/3000000 is </s>\n",
      "word #1/3000000 is in\n",
      "word #2/3000000 is for\n",
      "word #3/3000000 is that\n",
      "word #4/3000000 is is\n",
      "word #5/3000000 is on\n",
      "word #6/3000000 is ##\n",
      "word #7/3000000 is The\n",
      "word #8/3000000 is with\n",
      "word #9/3000000 is said\n"
     ]
    }
   ],
   "source": [
    "vec_king = model.wv['king']\n",
    "\n",
    "for index, word in enumerate(wv.index_to_key):\n",
    "    if index == 10:\n",
    "        break;\n",
    "    print(f\"word #{index}/{len(wv.index_to_key)} is {word}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de291184",
   "metadata": {},
   "source": [
    "## 存储和加载模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a2dc82f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tempfile\n",
    "\n",
    "with tempfile.NamedTemporaryFile(prefix='gensim-model-', delete=False) as tmp:\n",
    "    temporary_filepath = tmp.name\n",
    "    # 保存模型\n",
    "    model.save(temporary_filepath)\n",
    "    # 删除模型\n",
    "    new_model = gensim.models.Word2Vec.load(temporary_filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9f9b11a",
   "metadata": {},
   "source": [
    "##  训练你参数\n",
    "\n",
    "min_count: 有些单词出现的次数不足以了解他的含义\n",
    "\n",
    "vector_size: 词向量的维度大小"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6c2303c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
