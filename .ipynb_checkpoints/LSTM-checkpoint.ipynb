{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0a7d192e",
   "metadata": {},
   "source": [
    "项目地址: https://github.com/Edward1Chou/SentimentAnalysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0edc10fa",
   "metadata": {},
   "source": [
    "# 加载数据\n",
    "1. 通过pandas加载数据\n",
    "2. 通过jieba分词\n",
    "3. 通过gensim训练word2vec模型\n",
    "4. 创建字典"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "276df009",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import jieba\n",
    "\n",
    "# 从csv中载入数据\n",
    "def load_data():\n",
    "    # 通过pandas来处理csv文件中的数据\n",
    "    neg = pd.read_csv('data/neg.csv', header=None, index_col=None)\n",
    "    # 设置error_bad_lines=Flase 那么将丢弃读取错误的行\n",
    "    pos = pd.read_csv('data/pos.csv', header=None, index_col=None, error_bad_lines=False)\n",
    "    neu = pd.read_csv('data/neutral.csv', header=None, index_col=None)\n",
    "    \n",
    "    # 将上面3组数据合并在1个数组中\n",
    "    # combined长度=上述三者之和\n",
    "    combined = np.concatenate((pos[0], neu[0], neg[0]))\n",
    "    # 生成数据集对应的one-hot编码\n",
    "    y = np.concatenate((2*np.ones(len(pos), dtype=int), \n",
    "                       np.ones(len(neu), dtype=int), \n",
    "                       np.zeros(len(neg), dtype=int)))\n",
    "    return combined, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dbd0cede",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 通过结巴分词 将句子进行分词\n",
    "def tokenizer(text):\n",
    "    text = [jieba.lcut(document.replace('\\n', '')) for document in text]\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "980ac3bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 测试函数\n",
    "# combined, y = load_data()\n",
    "# combined = tokenizer(combined)\n",
    "# print(combined[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fbc1f062",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建字典\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "import multiprocessing\n",
    "\n",
    "# 词汇数量\n",
    "vocab_dim = 100\n",
    "# 设置出现频率的最低点\n",
    "n_exposures = 10\n",
    "# 根据上下文判断的窗口大小\n",
    "window_size = 7\n",
    "# 迭代次数\n",
    "n_iterations = 10\n",
    "# 工作类型\n",
    "cpu_count = multiprocessing.cpu_count()\n",
    "\n",
    "def create_dictionaries(model:Word2Vec=None, combined=None):\n",
    "    if (model==None) or (len(combined)==0):\n",
    "        print(\"没有提供数据\")\n",
    "        return\n",
    "    \n",
    "    gensim_dict = Dictionary()\n",
    "    # document->bag of words(bow)\n",
    "    # gensim删除了vocab属性 应该使用index_to_key\n",
    "    # allow_update 通过添加新的单词更新内部corpora的统计数据\n",
    "    gensim_dict.doc2bow(model.wv.index_to_key, allow_update=True)\n",
    "    \n",
    "    # 建立单词与词的转移表\n",
    "    word2index = {v:k+1 for k, v in gensim_dict.items()}\n",
    "    # 建立单词与向量的转移表\n",
    "    word2vector = {word:model.wv[word] for word in word2index.keys()}\n",
    "    \n",
    "    # 将combined中的单词转换成对应的index\n",
    "    def parse_dataset(combined):\n",
    "        data = []\n",
    "        for sentence in combined:\n",
    "            new_text = []\n",
    "            for word in sentence:\n",
    "                try:\n",
    "                    new_text.append(word2index[word])\n",
    "                except:\n",
    "                    new_text.append(0)\n",
    "            data.append(new_text)\n",
    "        return data\n",
    "        \n",
    "    combined = parse_dataset(combined)\n",
    "    \n",
    "    return word2index, word2vector, combined\n",
    "    \n",
    "# input: combined 是分词的集合\n",
    "# ouput: index_dict word->index\n",
    "# word_vectors word->vectors\n",
    "# combined 记录每个句子中单词的index值\n",
    "def word2vec_train(combined):\n",
    "    # 设置模型的训练参数\n",
    "    model = Word2Vec(vector_size=vocab_dim,\n",
    "                      min_count = n_exposures,\n",
    "                      window=window_size,\n",
    "                      workers=cpu_count)\n",
    "    # 训练模型\n",
    "    model.build_vocab(combined)\n",
    "    model.train(combined, total_examples=model.corpus_count, epochs=n_iterations)\n",
    "    # 保存模型\n",
    "    model.save('./model/Word2Vec_model.pkl')\n",
    "    # 通过创建映射关系\n",
    "    index_dict, word_vectors, combined = create_dictionaries(model=model, combined=combined)\n",
    "    \n",
    "    return index_dict, word_vectors, combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "254e63b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1892588/129182137.py:10: FutureWarning: The error_bad_lines argument has been deprecated and will be removed in a future version. Use on_bad_lines in the future.\n",
      "\n",
      "\n",
      "  pos = pd.read_csv('data/pos.csv', header=None, index_col=None, error_bad_lines=False)\n",
      "Skipping line 2607: expected 1 fields, saw 9\n",
      "Skipping line 3143: expected 1 fields, saw 2\n",
      "Skipping line 3173: expected 1 fields, saw 8\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 加载数据\n",
    "sentences, y = load_data()\n",
    "combined = tokenizer(sentences)\n",
    "\n",
    "# 训练word2Vec模型\n",
    "# word2vec_train(combined)\n",
    "\n",
    "# 加载模型\n",
    "model = Word2Vec.load('./model/Word2Vec_model.pkl')\n",
    "\n",
    "# print(model.wv.index_to_key)\n",
    "index_dict, word_vectors, combined = create_dictionaries(model, combined)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cef83e9",
   "metadata": {},
   "source": [
    "## 得到数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c803d1c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 通过pytorch 建立模型\n",
    "# https://pytorch.org/tutorials/beginner/basics/quickstart_tutorial.html\n",
    "\n",
    "# lstm使用 https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html#torch.nn.LSTM\n",
    "import torch\n",
    "import torch.utils.data as data\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "device=torch.device(\"cuda\")\n",
    "\n",
    "def get_data(index_dict, word_vectors, combined, y):\n",
    "    # 所有单词的索引数量\n",
    "    n_symbols = len(index_dict) + 1\n",
    "    \n",
    "    # 初始化词向量\n",
    "    embedding_weights = torch.zeros((n_symbols, vocab_dim))\n",
    "    for word, index in index_dict.items():\n",
    "        embedding_weights[index, :] = torch.from_numpy(word_vectors[word])\n",
    "    \n",
    "    # 将数据集转换为tensor形式\n",
    "    print(combined)\n",
    "    combined = pad_sequence([torch.tensor(doc, dtype=torch.long, device=device) for doc in combined], batch_first=True)\n",
    "    y = torch.tensor(y, dtype=torch.long, device=device)\n",
    "    \n",
    "    # 将数据集拆分成train/valid\n",
    "    test_rate = 0.2\n",
    "    test_size = int(len(combined) * test_rate)\n",
    "    train_size = len(combined) - test_size\n",
    "    datasets = data.random_split(data.TensorDataset(combined, y), [train_size, test_size])\n",
    "    train_ds = datasets[0]\n",
    "    valid_ds = datasets[1]\n",
    "    return n_symbols, embedding_weights, train_ds, valid_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d274ecf",
   "metadata": {},
   "source": [
    "## 建立模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d46c561c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# 建立一个模型\n",
    "class MyModel(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super().__init__()\n",
    "        # 关于batch_first使用: https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html#torch.nn.LSTM\n",
    "        # 这里是根据第一个维度来进行批量操作\n",
    "        self.embbed = nn.Embedding(n_symbols, vocab_dim).from_pretrained(embedding_weights)\n",
    "        self.lstm = nn.LSTM(input_size=vocab_dim*input_size,hidden_size=50, num_layers=1, batch_first=True)\n",
    "        self.lin1 = nn.Linear(50, 50)\n",
    "#         self.lin2 = nn.Linear(50, 50)\n",
    "        self.lin3 = nn.Linear(50, 3)\n",
    "    def forward(self, x):\n",
    "        x = self.embbed(x).view(x.shape[0], -1)\n",
    "        x, _ = self.lstm(x)\n",
    "        x = F.relu(self.lin1(x))\n",
    "#         x = F.relu(self.lin2(x))\n",
    "        x = self.lin3(x)\n",
    "        # softmax + log\n",
    "        x = F.softmax(x, dim=1)\n",
    "        return x\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "8a86a65b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[52], line 53\u001b[0m\n\u001b[1;32m     49\u001b[0m     torch\u001b[38;5;241m.\u001b[39msave(myModel\u001b[38;5;241m.\u001b[39mstate_dict(), PATH)\n\u001b[1;32m     52\u001b[0m \u001b[38;5;66;03m# 可以确定一件事 index_dict中没有0\u001b[39;00m\n\u001b[0;32m---> 53\u001b[0m n_symbols, embedding_weights, train_ds, valid_ds \u001b[38;5;241m=\u001b[39m \u001b[43mget_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mword_vectors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcombined\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m train_lstm(n_symbols, embedding_weights, train_ds, valid_ds)\n",
      "Cell \u001b[0;32mIn[7], line 24\u001b[0m, in \u001b[0;36mget_data\u001b[0;34m(index_dict, word_vectors, combined, y)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# 将数据集转换为tensor形式\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28mprint\u001b[39m(combined)\n\u001b[0;32m---> 24\u001b[0m combined \u001b[38;5;241m=\u001b[39m pad_sequence([torch\u001b[38;5;241m.\u001b[39mtensor(doc, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong, device\u001b[38;5;241m=\u001b[39mdevice) \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m combined], batch_first\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     25\u001b[0m y \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(y, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong, device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# 将数据集拆分成train/valid\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[7], line 24\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# 将数据集转换为tensor形式\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28mprint\u001b[39m(combined)\n\u001b[0;32m---> 24\u001b[0m combined \u001b[38;5;241m=\u001b[39m pad_sequence([\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlong\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m combined], batch_first\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     25\u001b[0m y \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(y, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong, device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# 将数据集拆分成train/valid\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "input_size = 1804\n",
    "\n",
    "\n",
    "\n",
    "def train_lstm(n_symbols, embedding_weights, train_dataset, valid_dataset, is_print=False):\n",
    "    if is_print:\n",
    "        print('Defining a Simple Torch Model...')\n",
    "    \n",
    "    # https://pytorch.org/tutorials/beginner/basics/buildmodel_tutorial.html\n",
    "#     model = nn.Sequential(\n",
    "#             nn.Embedding(n_symbols, vocab_dim).from_pretrained(embedding_weights),\n",
    "#             nn.LSTM(vocab_dim, 50, batch_first=True),\n",
    "#             nn.Linear(50, 50),\n",
    "#             nn.Linear(50, 50),\n",
    "#             nn.Linear(50, 50),\n",
    "#             nn.Softmax(3),\n",
    "#     )\n",
    "    myModel = MyModel(1804)\n",
    "    myModel.to(device)\n",
    "    \n",
    "    optimizer = optim.Adam(myModel.parameters())\n",
    "    loss_func = nn.CrossEntropyLoss()\n",
    "    loader = data.DataLoader(train_dataset, shuffle=True, batch_size=32)\n",
    "    \n",
    "    epochs = 200\n",
    "    for epoch in range(epochs):\n",
    "        myModel.train()\n",
    "        \n",
    "        for X_batch, y_batch in loader:\n",
    "        \n",
    "            y_pred = myModel(X_batch)\n",
    "            loss = loss_func(y_pred, y_batch)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "        if epoch % 100 == 0:\n",
    "            myModel.eval()\n",
    "            with torch.no_grad():\n",
    "                valid_loader = data.DataLoader(valid_dataset, shuffle=True, batch_size=32)\n",
    "                losses = torch.tensor([loss_func(myModel(x_valid), y_valid) for x_valid, y_valid in valid_loader ])\n",
    "                loss = torch.mean(losses)\n",
    "                if is_print:\n",
    "                    print(epoch, loss)\n",
    "    # 保存模型\n",
    "    PATH = \"./model/lstm.model\"\n",
    "    torch.save(myModel.state_dict(), PATH)\n",
    "            \n",
    "\n",
    "# 可以确定一件事 index_dict中没有0\n",
    "n_symbols, embedding_weights, train_ds, valid_ds = get_data(index_dict, word_vectors, combined, y)\n",
    "\n",
    "train_lstm(n_symbols, embedding_weights, train_ds, valid_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceced71e",
   "metadata": {},
   "source": [
    "计数数据集\n",
    "\n",
    "https://blog.csdn.net/m0_37694033/article/details/121977987\n",
    "\n",
    "https://blog.csdn.net/buluxianfeng/article/details/125731803?spm=1001.2101.3001.6650.1&utm_medium=distribute.pc_relevant.none-task-blog-2%7Edefault%7ECTRLIST%7ERate-1-125731803-blog-121977987.235%5Ev30%5Epc_relevant_default_base3&depth_1-utm_source=distribute.pc_relevant.none-task-blog-2%7Edefault%7ECTRLIST%7ERate-1-125731803-blog-121977987.235%5Ev30%5Epc_relevant_default_base3&utm_relevant_index=2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "431f6fe3",
   "metadata": {},
   "source": [
    "## 预测部分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "7c6a7b4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load model...\n",
      "太差劲了   negative\n",
      "load model...\n",
      "好极了   positive\n",
      "load model...\n",
      "声音质量很差，摄像头效果也很差   negative\n",
      "load model...\n",
      "音质很好，电池不错，设计大方，还是物有所值。   positive\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def input_transform(string):\n",
    "    words = jieba.lcut(string)\n",
    "    words = np.array(words).reshape(1, -1)\n",
    "    model = Word2Vec.load('./model/Word2Vec_model.pkl')\n",
    "    _, _, combined = create_dictionaries(model, words)\n",
    "    return combined\n",
    "\n",
    "def lstm_predict(string):\n",
    "    combined = input_transform(string)\n",
    "#     print(combined)\n",
    "    \n",
    "    combined[0] = np.append(combined[0], np.zeros(input_size - len(combined[0])))\n",
    "    \n",
    "    combined = pad_sequence([torch.tensor(doc, dtype=torch.long, device=device) for doc in combined], batch_first=True)\n",
    "#     y = torch.tensor(y, dtype=torch.long, device=device)\n",
    "    \n",
    "    model = MyModel(1804)\n",
    "    model.load_state_dict(torch.load(PATH, map_location=device))\n",
    "    model.to(device)\n",
    "    \n",
    "    result = model(combined)\n",
    "    _, res = torch.max(result, 1)\n",
    "    if res == 0:\n",
    "        return \"negative\"\n",
    "    elif res == 1:\n",
    "        return \"neutral\"\n",
    "    elif res == 2:\n",
    "        return \"positive\"\n",
    "\n",
    "# for sentence in sentences:\n",
    "#     lstm_predict(sentence)\n",
    "\n",
    "# print(sentences[0])\n",
    "# print(y[0])\n",
    "\n",
    "sentences = [\"太差劲了\", \"好极了\", \"声音质量很差，摄像头效果也很差\", \"音质很好，电池不错，设计大方，还是物有所值。\"]\n",
    "\n",
    "for sentence in sentences:\n",
    "    print(sentence, ' ', lstm_predict(sentence))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b8a81e7",
   "metadata": {},
   "source": [
    "模型的输入是1804，但是测试集的长度小于1804"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bfdfd96",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b3f6580",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
