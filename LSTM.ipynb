{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0a7d192e",
   "metadata": {},
   "source": [
    "项目地址: https://github.com/Edward1Chou/SentimentAnalysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0edc10fa",
   "metadata": {},
   "source": [
    "# 读取数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "276df009",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import jieba\n",
    "\n",
    "# 从csv中载入数据\n",
    "def load_data():\n",
    "    # 通过pandas来处理csv文件中的数据\n",
    "    neg = pd.read_csv('data/neg.csv', header=None, index_col=None)\n",
    "    # 设置error_bad_lines=Flase 那么将丢弃读取错误的行\n",
    "    pos = pd.read_csv('data/pos.csv', header=None, index_col=None, error_bad_lines=False)\n",
    "    neu = pd.read_csv('data/neutral.csv', header=None, index_col=None)\n",
    "    \n",
    "    # 将上面3组数据合并在1个数组中\n",
    "    # combined长度=上述三者之和\n",
    "    combined = np.concatenate((pos[0], neu[0], neg[0]))\n",
    "    # 生成数据集对应的onthot编码\n",
    "    y = np.concatenate((np.ones(len(pos), dtype=int), \n",
    "                       np.zeros(len(neu), dtype=int), \n",
    "                       -1*np.ones(len(neg), dtype=int)))\n",
    "    return combined, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dbd0cede",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 通过结巴分词 将句子进行分词\n",
    "def tokenizer(text):\n",
    "    text = [jieba.lcut(document.replace('\\n', '')) for document in text]\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "980ac3bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 测试函数\n",
    "# combined, y = load_data()\n",
    "# combined = tokenizer(combined)\n",
    "# print(combined[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "fbc1f062",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建字典\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "import multiprocessing\n",
    "\n",
    "# 词汇数量\n",
    "vocab_dim = 100\n",
    "# 设置出现频率的最低点\n",
    "n_exposures = 10\n",
    "# 根据上下文判断的窗口大小\n",
    "window_size = 7\n",
    "# 迭代次数\n",
    "n_iterations = 10\n",
    "# 工作类型\n",
    "cpu_count = multiprocessing.cpu_count()\n",
    "\n",
    "def create_dictionaries(model:Word2Vec=None, combined=None):\n",
    "    if (model==None) or (combined==None):\n",
    "        print(\"没有提供数据\")\n",
    "        return\n",
    "    \n",
    "    gensim_dict = Dictionary()\n",
    "    # document->bag of words(bow)\n",
    "    # gensim删除了vocab属性 应该使用index_to_key\n",
    "    # allow_update 通过添加新的单词更新内部corpora的统计数据\n",
    "    gensim_dict.doc2bow(model.wv.index_to_key, allow_update=True)\n",
    "    \n",
    "    # 建立单词与词的转移表\n",
    "    word2index = {v:k+1 for k, v in gensim_dict.items()}\n",
    "    # 建立单词与向量的转移表\n",
    "    word2vector = {word:model.wv[word] for word in word2index.keys()}\n",
    "    \n",
    "    # 将combined中的单词转换成对应的index\n",
    "    def parse_dataset(combined):\n",
    "        data = []\n",
    "        for sentence in combined:\n",
    "            new_text = []\n",
    "            for word in sentence:\n",
    "                try:\n",
    "                    new_text.append(word2index[word])\n",
    "                except:\n",
    "                    new_text.append(0)\n",
    "            data.append(new_text)\n",
    "        return data\n",
    "        \n",
    "    combined = parse_dataset(combined)\n",
    "    \n",
    "    return word2index, word2vector, combined\n",
    "    \n",
    "# input: combined 是分词的集合\n",
    "# ouput: index_dict word->index\n",
    "# word_vectors word->vectors\n",
    "# combined 记录每个句子中单词的index值\n",
    "def word2vec_train(combined):\n",
    "    # 设置模型的训练参数\n",
    "    model = Word2Vec(vector_size=vocab_dim,\n",
    "                      min_count = n_exposures,\n",
    "                      window=window_size,\n",
    "                      workers=cpu_count)\n",
    "    # 训练模型\n",
    "    model.build_vocab(combined)\n",
    "    model.train(combined, total_examples=model.corpus_count, epochs=n_iterations)\n",
    "    # 保存模型\n",
    "    model.save('./model/Word2Vec_model.pkl')\n",
    "    # 通过创建映射关系\n",
    "    index_dict, word_vectors, combined = create_dictionaries(model=model, combined=combined)\n",
    "    \n",
    "    return index_dict, word_vectors, combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "254e63b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 测试函数\n",
    "# word2vec_train(combined)\n",
    "\n",
    "# model = Word2Vec.load('./model/Word2Vec_model.pkl')\n",
    "# print(model.wv.index_to_key)\n",
    "# create_dictionaries(model, combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c803d1c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 通过pytorch 建立模型\n",
    "# https://pytorch.org/tutorials/beginner/basics/quickstart_tutorial.html\n",
    "\n",
    "\n",
    "# lstm使用 https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html#torch.nn.LSTM\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "model = nn.Sequential(\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d1f1c3b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
