{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0a7d192e",
   "metadata": {},
   "source": [
    "项目地址: https://github.com/Edward1Chou/SentimentAnalysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0edc10fa",
   "metadata": {},
   "source": [
    "# 读取数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "276df009",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import jieba\n",
    "\n",
    "# 从csv中载入数据\n",
    "def load_data():\n",
    "    # 通过pandas来处理csv文件中的数据\n",
    "    neg = pd.read_csv('data/neg.csv', header=None, index_col=None)\n",
    "    # 设置error_bad_lines=Flase 那么将丢弃读取错误的行\n",
    "    pos = pd.read_csv('data/pos.csv', header=None, index_col=None, error_bad_lines=False)\n",
    "    neu = pd.read_csv('data/neutral.csv', header=None, index_col=None)\n",
    "    \n",
    "    # 将上面3组数据合并在1个数组中\n",
    "    # combined长度=上述三者之和\n",
    "    combined = np.concatenate((pos[0], neu[0], neg[0]))\n",
    "    # 生成数据集对应的onthot编码\n",
    "    y = np.concatenate((np.ones(len(pos), dtype=int), \n",
    "                       np.zeros(len(neu), dtype=int), \n",
    "                       -1*np.ones(len(neg), dtype=int)))\n",
    "    return combined, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dbd0cede",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 通过结巴分词 将句子进行分词\n",
    "def tokenizer(text):\n",
    "    text = [jieba.lcut(document.replace('\\n', '')) for document in text]\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "980ac3bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 测试函数\n",
    "# combined, y = load_data()\n",
    "# combined = tokenizer(combined)\n",
    "# print(combined[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "fbc1f062",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建字典\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "import multiprocessing\n",
    "\n",
    "# 词汇数量\n",
    "vocab_dim = 100\n",
    "# 设置出现频率的最低点\n",
    "n_exposures = 10\n",
    "# 根据上下文判断的窗口大小\n",
    "window_size = 7\n",
    "# 迭代次数\n",
    "n_iterations = 10\n",
    "# 工作类型\n",
    "cpu_count = multiprocessing.cpu_count()\n",
    "\n",
    "def create_dictionaries(model:Word2Vec=None, combined=None):\n",
    "    if (model==None) or (combined==None):\n",
    "        print(\"没有提供数据\")\n",
    "        return\n",
    "    \n",
    "    gensim_dict = Dictionary()\n",
    "    # document->bag of words(bow)\n",
    "    # gensim删除了vocab属性 应该使用index_to_key\n",
    "    # allow_update 通过添加新的单词更新内部corpora的统计数据\n",
    "    gensim_dict.doc2bow(model.wv.index_to_key, allow_update=True)\n",
    "    \n",
    "    # 建立单词与词的转移表\n",
    "    word2index = {v:k+1 for k, v in gensim_dict.items()}\n",
    "    # 建立单词与向量的转移表\n",
    "    word2vector = {word:model.wv[word] for word in word2index.keys()}\n",
    "    \n",
    "    # 将combined中的单词转换成对应的index\n",
    "    def parse_dataset(combined):\n",
    "        data = []\n",
    "        for sentence in combined:\n",
    "            new_text = []\n",
    "            for word in sentence:\n",
    "                try:\n",
    "                    new_text.append(word2index[word])\n",
    "                except:\n",
    "                    new_text.append(0)\n",
    "            data.append(new_text)\n",
    "        return data\n",
    "        \n",
    "    combined = parse_dataset(combined)\n",
    "    \n",
    "    return word2index, word2vector, combined\n",
    "    \n",
    "# input: combined 是分词的集合\n",
    "# ouput: index_dict word->index\n",
    "# word_vectors word->vectors\n",
    "# combined 记录每个句子中单词的index值\n",
    "def word2vec_train(combined):\n",
    "    # 设置模型的训练参数\n",
    "    model = Word2Vec(vector_size=vocab_dim,\n",
    "                      min_count = n_exposures,\n",
    "                      window=window_size,\n",
    "                      workers=cpu_count)\n",
    "    # 训练模型\n",
    "    model.build_vocab(combined)\n",
    "    model.train(combined, total_examples=model.corpus_count, epochs=n_iterations)\n",
    "    # 保存模型\n",
    "    model.save('./model/Word2Vec_model.pkl')\n",
    "    # 通过创建映射关系\n",
    "    index_dict, word_vectors, combined = create_dictionaries(model=model, combined=combined)\n",
    "    \n",
    "    return index_dict, word_vectors, combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "254e63b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Fucloud\\AppData\\Local\\Temp\\ipykernel_18744\\4233722029.py:10: FutureWarning: The error_bad_lines argument has been deprecated and will be removed in a future version. Use on_bad_lines in the future.\n",
      "\n",
      "\n",
      "  pos = pd.read_csv('data/pos.csv', header=None, index_col=None, error_bad_lines=False)\n",
      "Skipping line 2607: expected 1 fields, saw 9\n",
      "Skipping line 3143: expected 1 fields, saw 2\n",
      "Skipping line 3173: expected 1 fields, saw 8\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary<8308 unique tokens: [' ', '!', '\"', '#', '$']...>\n"
     ]
    }
   ],
   "source": [
    "# 测试函数\n",
    "# word2vec_train(combined)\n",
    "\n",
    "combined, y = load_data()\n",
    "combined = tokenizer(combined)\n",
    "model = Word2Vec.load('./model/Word2Vec_model.pkl')\n",
    "# print(model.wv.index_to_key)\n",
    "index_dict, word_vectors, combined = create_dictionaries(model, combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "c803d1c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 通过pytorch 建立模型\n",
    "# https://pytorch.org/tutorials/beginner/basics/quickstart_tutorial.html\n",
    "\n",
    "\n",
    "# lstm使用 https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html#torch.nn.LSTM\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.utils.data as data\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "def get_data(index_dict, word_vectors, combined, y):\n",
    "    # 所有单词的索引数量\n",
    "    n_symbols = len(index_dict) + 1\n",
    "    \n",
    "    # 初始化词向量\n",
    "    embedding_weights = torch.zeros((n_symbols, vocab_dim))\n",
    "    for word, index in index_dict.items():\n",
    "        embedding_weights[index, :] = torch.from_numpy(word_vectors[word])\n",
    "    \n",
    "    # 将数据集转换为tensor形式\n",
    "    combined = pad_sequence([torch.tensor(doc) for doc in combined], batch_first=True)\n",
    "    y = torch.tensor(y)\n",
    "    \n",
    "    # 将数据集拆分成train/valid\n",
    "    test_rate = 0.2\n",
    "    test_size = int(len(combined) * test_rate)\n",
    "    train_size = len(combined) - test_size\n",
    "    datasets = data.random_split(data.TensorDataset(combined, y), [train_size, test_size])\n",
    "    train_ds = datasets[0]\n",
    "    valid_ds = datasets[1]\n",
    "    return n_symbols, embedding_weights, train_ds, valid_ds\n",
    "\n",
    "def train_lstm(n_symbols, embedding_weights, train_dataset, valid_dataset):\n",
    "    print('Defining a Simple Torch Model...')\n",
    "    \n",
    "    # https://pytorch.org/tutorials/beginner/basics/buildmodel_tutorial.html\n",
    "    model = nn.Sequential(\n",
    "            nn.Embedding(n_symbols, vocab_dim).from_pretrained(embedding_weights),\n",
    "            nn.LSTM(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(3),\n",
    "            nn.Softmax(3),\n",
    "    )\n",
    "    \n",
    "    optimizer = optim.Adam()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "2d1f1c3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defining a Simple Torch Model...\n"
     ]
    }
   ],
   "source": [
    "# 可以确定一件事 index_dict中没有0\n",
    "\n",
    "n_symbols, embedding_weights, train_ds, valid_ds = get_data(index_dict, word_vectors, combined, y)\n",
    "\n",
    "train_lstm(n_symbols, embedding_weights, train_ds, valid_ds)\n",
    "\n",
    "\n",
    "# embedding = nn.Embedding(num_embeddings=n_symbols, \n",
    "#                          embedding_dim=vocab_dim).from_pretrained(embedding_weight)\n",
    "# print(embedding.weight.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dca64815",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f59fedbe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
